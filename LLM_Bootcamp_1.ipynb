{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0KGCJivZvDY+rmF3kuAGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1nancan/llm_intro/blob/main/LLM_Bootcamp_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vku6Ml2j7JrR"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain\n",
        "#!pip install openai\n",
        "#!pip install -U langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "wHubsYz27K4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature = 0, openai_api_key=userdata.get('open_ai'))"
      ],
      "metadata": {
        "id": "oLMZLsCw7STW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"hello, this is a test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NAQWKwSA7fob",
        "outputId": "b7650698-d9ad-4cb6-cbf5-b684e69923bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\nHello! Thank you for letting me know. Is there anything else you would like to test or discuss?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(llm(\"hello, this is a test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "GCC92uG29z-S",
        "outputId": "a46d3905-1826-4aef-b192-da4cc1011d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n\nHello! Thank you for letting me know. Is there anything else you would like to test or discuss?"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(llm(\"What is zero-shot chain-of-though prompting\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "ikpH20Lm_xCO",
        "outputId": "85a2da9a-3f53-4326-ac79-bfcdc75c672c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nZero-shot chain-of-thought prompting is a natural language processing technique that involves generating a sequence of words or phrases based on a given prompt or starting point. Unlike traditional language models, which require large amounts of training data and specific prompts to generate coherent text, zero-shot chain-of-thought prompting allows for the generation of text without any specific training on the given prompt. This is achieved by using a pre-trained language model that has been trained on a wide range of text data and can generate text based on the context and relationships between words and phrases. This technique is useful for tasks such as text summarization, question-answering, and text completion, where the model can generate relevant and coherent text without being explicitly trained on the specific task or prompt."
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-VZnosbApcg",
        "outputId": "96be2de2-f27d-4f8a-bda2-e03a14c190a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/81.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "search_query = arxiv.Search(id_list=[\"2205.11916\"])\n",
        "\n",
        "# Create an arXiv client\n",
        "client = arxiv.Client()\n",
        "\n",
        "# Get the results of the query\n",
        "results = client.results(search_query)\n",
        "paper = next(results)\n",
        "Markdown(paper.summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "0v5EbtgcCch7",
        "outputId": "8fa126e5-9db0-4e11-8f3e-0ee0b029d960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars."
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(f\"\"\"Here's a summary of a paper:\n",
        "{paper.summary}\n",
        "\n",
        "Based on that summary, what is zero-shot chain-of-thought prompting?\"\"\")\n",
        "\n",
        "Markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "zqeG8SxFA89q",
        "outputId": "94ce2b4f-ad4c-4f6c-ffda-6a1d746bb67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nZero-shot chain-of-thought prompting is a technique that utilizes large language models (LLMs) to perform complex multi-step reasoning tasks without any hand-crafted few-shot examples. It involves adding the phrase \"Let's think step by step\" before each answer in a prompt, which significantly improves the zero-shot performance of LLMs on diverse reasoning tasks such as arithmetics, symbolic reasoning, and logical reasoning. This technique suggests that LLMs have untapped and understudied zero-shot capabilities, and highlights the importance of exploring and analyzing these capabilities before creating finetuning datasets or few-shot exemplars."
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make the paper searchable"
      ],
      "metadata": {
        "id": "i5RRUC4CDMZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -qqq pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZOyACchCvVx",
        "outputId": "8c399007-7291-485f-a22f-8f9d3eb60929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/278.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/278.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m276.5/278.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_path = paper.download_pdf()"
      ],
      "metadata": {
        "id": "VarvNOGmDRTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(paper_path)\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "ftkbJVhNDWS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyfUa9Y6Df3L",
        "outputId": "fa41ecde-f68a-4717-9733-96650c056ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = \"\\n\\n\".join([page.page_content for page in pages[0:2]])"
      ],
      "metadata": {
        "id": "j7QVMhRPDiyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(f\"\"\"Here's the first two pages of a paper:\n",
        "{content}\n",
        "\n",
        "Based on that content, what is zero-shot chain-of-thought prompting?\"\"\")\n",
        "\n",
        "Markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "7JEBHFukDoye",
        "outputId": "a3c6ec2f-5756-4e51-ee9b-d105c80d28bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nZero-shot chain-of-thought prompting is a technique for eliciting complex multi-step reasoning through step-by-step answer examples, without the need for task-specific few-shot examples. It involves adding the prompt \"Let's think step by step\" before each answer in order to facilitate step-by-step thinking and improve the performance of large language models on challenging reasoning tasks. This approach is shown to be effective in a variety of tasks, including arithmetic, symbolic, commonsense, and other logical reasoning tasks."
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find relevant content using embedding search"
      ],
      "metadata": {
        "id": "PVKxhDNYD-I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq faiss-cpu tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YLk15Y6D2QR",
        "outputId": "c281a9a2-f911-463c-9f9b-bf42231dcc4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "KtktGlVbEDL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=userdata.get('open_ai'))\n",
        "\n",
        "db = FAISS.from_documents(docs,embeddings)"
      ],
      "metadata": {
        "id": "dk4s15oZEOrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = db.similarity_search(\"What is zero-shot chain-of-though prompting?\")"
      ],
      "metadata": {
        "id": "tm9OCIS2EfVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "KZrO1OLXEsTi",
        "outputId": "4c174c12-8170-4a79-ea6a-0d1a5b68a2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "language models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\ngiven for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\nasFew-shot-CoT in this work.\n3 Zero-shot Chain of Thought\nWe propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\nIt differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\nstep-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\n2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\nwith a single template. The core idea of our method is simple, as described in Figure 1: add Let’s"
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use an existing chain"
      ],
      "metadata": {
        "id": "qdIVXuGTE36m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
      ],
      "metadata": {
        "id": "kzyCCrC_EvZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
        "query = \"what is zero-shot chain-of-though prompting?\"\n",
        "sources = db.similarity_search(query)\n",
        "results = chain({\"input_documents\": sources, \"question\":query},return_only_outputs = True)\n"
      ],
      "metadata": {
        "id": "LBcaSFNYE96k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(results[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "vN4osyi1FQSO",
        "outputId": "261bb0cc-7ae6-46af-bdbc-7ecf39db3256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Zero-shot chain-of-thought prompting is a method of prompting large language models to perform complex reasoning tasks without the need for in-context learning or hand-crafted few-shot examples. It differs from previous prompting methods and has been shown to improve the zero-shot reasoning ability of LLMs across a variety of tasks. \nSOURCES: ./2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf"
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAKE A DISCORD BOT THAT RETURNS THE SUMMARY OF A ARXIV PAPER / PDF."
      ],
      "metadata": {
        "id": "vPIDEJS4FUoz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}